{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\nclass AllMpnetBaseV2:\n    def __init__(self):\n        self.sentences = [\"This is a sentence\", \"This is another sentence\"]\n        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n        self.model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n\n    def calculate_cosine_similarity_between_sentences(self, input_sentences=None):\n        if input_sentences is None:\n            input_sentences = self.sentences\n        \n        if not isinstance(input_sentences, list) or len(input_sentences) != 2:\n            raise ValueError(\"Exactly 2 sentences are required for comparison.\")\n        \n        encoded_embeddings = self.__get_normalized_sentence_embeddings(input_sentences)\n        embeddings_prepared_for_similarity = [embedding.view(1, -1) for embedding in encoded_embeddings]\n        cosine_similarity_score = F.cosine_similarity(embeddings_prepared_for_similarity[0], embeddings_prepared_for_similarity[1])\n        return cosine_similarity_score.item()\n\n    def find_highest_cosine_similarity_pair(self, input_sentences):\n        embeddings = self.__encode_and_normalize_sentences(input_sentences)\n        highest_similarity_score = float('-inf')\n        most_similar_sentence_pair = (None, None)\n\n        total_sentences = len(input_sentences)\n        for i in range(total_sentences):\n            for j in range(i + 1, total_sentences):\n                current_similarity = F.cosine_similarity(embeddings[i].unsqueeze(0), embeddings[j].unsqueeze(0)).item()\n\n                if current_similarity > highest_similarity_score:\n                    highest_similarity_score = current_similarity\n                    most_similar_sentence_pair = (input_sentences[i], input_sentences[j])\n\n        return highest_similarity_score, most_similar_sentence_pair\n\n    def process_and_normalize_sentence_embedding(self, input_sentence):\n        tokenized_input = self.tokenizer(input_sentence, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            output_from_model = self.model(**tokenized_input)\n\n        pooled_embedding = self.__apply_mean_pooling_to_embedding(output_from_model, tokenized_input['attention_mask'])\n        normalized_embedding = F.normalize(pooled_embedding, p=2, dim=1)\n        return normalized_embedding.view(-1).tolist()\n\n    def __apply_mean_pooling_to_embedding(self, output_from_model, attention_mask):\n        embeddings_of_tokens = output_from_model.last_hidden_state\n        expanded_attention_mask = attention_mask.unsqueeze(-1).expand_as(embeddings_of_tokens).float()\n        weighted_sum_embeddings = torch.sum(embeddings_of_tokens * expanded_attention_mask, axis=1)\n        sum_mask = torch.clamp(expanded_attention_mask.sum(1), min=1e-9)\n        mean_pooled_embeddings = weighted_sum_embeddings / sum_mask\n        return mean_pooled_embeddings\n\n    def __encode_and_normalize_sentences(self, sentences):\n        tokenized_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n        with torch.no_grad():\n            model_output = self.model(**tokenized_input)\n\n        pooled_embeddings = self.__apply_mean_pooling_to_embedding(model_output, tokenized_input['attention_mask'])\n        normalized_embeddings = F.normalize(pooled_embeddings, p=2, dim=1)\n        return normalized_embeddings\n\n    def __get_normalized_sentence_embeddings(self, sentences):\n        # Tokenize sentences\n        tokenized_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n\n        # Generate embeddings\n        with torch.no_grad():\n            outputs = self.model(**tokenized_input)\n\n        # Apply mean pooling (assuming you have a method like __apply_mean_pooling_to_embedding)\n        embeddings = self.__apply_mean_pooling_to_embedding(outputs, tokenized_input['attention_mask'])\n\n        # Normalize embeddings\n        normalized_embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n\n        return normalized_embeddings\n\n# Now instantiate the class\nmodel_revised = AllMpnetBaseV2()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-26T14:59:00.452237Z","iopub.execute_input":"2024-02-26T14:59:00.452604Z","iopub.status.idle":"2024-02-26T14:59:00.906079Z","shell.execute_reply.started":"2024-02-26T14:59:00.452581Z","shell.execute_reply":"2024-02-26T14:59:00.904798Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"try:\n    model_revised = AllMpnetBaseV2()\n    print(\"Initialization successful. Model and tokenizer are loaded.\")\nexcept Exception as e:\n    print(f\"Initialization failed: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:59:05.641602Z","iopub.execute_input":"2024-02-26T14:59:05.641975Z","iopub.status.idle":"2024-02-26T14:59:06.109427Z","shell.execute_reply.started":"2024-02-26T14:59:05.641945Z","shell.execute_reply":"2024-02-26T14:59:06.108532Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Initialization successful. Model and tokenizer are loaded.\n","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    similarity_score = model_revised.calculate_cosine_similarity_between_sentences([\"This is a test sentence.\", \"This is another test sentence.\"])\n    print(f\"Cosine Similarity Score: {similarity_score}\")\nexcept Exception as e:\n    print(f\"Error calculating cosine similarity: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:59:09.231397Z","iopub.execute_input":"2024-02-26T14:59:09.231744Z","iopub.status.idle":"2024-02-26T14:59:09.335398Z","shell.execute_reply.started":"2024-02-26T14:59:09.231718Z","shell.execute_reply":"2024-02-26T14:59:09.334423Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Cosine Similarity Score: 0.9610860347747803\n","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    sentences = [\n        \"This is a test sentence.\",\n        \"This is another test sentence.\",\n        \"An entirely different sentence.\",\n        \"Yet another sentence for testing.\"\n    ]\n    highest_score, most_similar_pair = model_revised.find_highest_cosine_similarity_pair(sentences)\n    print(f\"Highest Cosine Similarity Score: {highest_score}, between sentences: {most_similar_pair}\")\nexcept Exception as e:\n    print(f\"Error finding highest cosine similarity pair: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:59:13.511186Z","iopub.execute_input":"2024-02-26T14:59:13.511495Z","iopub.status.idle":"2024-02-26T14:59:13.629984Z","shell.execute_reply.started":"2024-02-26T14:59:13.511474Z","shell.execute_reply":"2024-02-26T14:59:13.629351Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Highest Cosine Similarity Score: 0.9610860347747803, between sentences: ('This is a test sentence.', 'This is another test sentence.')\n","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    sentence_embedding = model_revised.process_and_normalize_sentence_embedding(\"This is a test sentence.\")\n    print(f\"Normalized Sentence Embedding: {sentence_embedding[:5]}... [truncated for brevity]\")\nexcept Exception as e:\n    print(f\"Error processing and normalizing sentence embedding: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:59:18.376821Z","iopub.execute_input":"2024-02-26T14:59:18.377316Z","iopub.status.idle":"2024-02-26T14:59:18.468112Z","shell.execute_reply.started":"2024-02-26T14:59:18.377248Z","shell.execute_reply":"2024-02-26T14:59:18.466567Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Normalized Sentence Embedding: [0.0003780757251661271, -0.05080346018075943, -0.035147227346897125, -0.023251069709658623, -0.04415823519229889]... [truncated for brevity]\n","output_type":"stream"}]}]}